{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_data(file):\n",
    "    data = load_svmlight_file(file)\n",
    "    return data[0], data[1]\n",
    "\n",
    "def compute_loss(X, W, y, C = 1):\n",
    "    L2 =  0.5 * np.dot(W.T, W)\n",
    "    prediction_y = np.dot(X, W)\n",
    "    #print(\"prediction_y\", prediction_y)\n",
    "    diff = np.ones(y.shape[0]) - y * prediction_y\n",
    "    diff[diff < 0] = 0\n",
    "    #print(\"diff\", diff)\n",
    "    hingeloss = C *(np.sum(diff)) / X.shape[0]\n",
    "    loss = hingeloss + L2\n",
    "    return loss\n",
    "\n",
    "def get_gradient(X, W, y, C = 1):\n",
    "    prediction_y = np.dot(X, W)\n",
    "    diff = np.ones(y.shape[0]) - np.multiply(y, prediction_y)\n",
    "    #print(\"diff\", diff.shape)\n",
    "    #print(\"y\", y.shape)\n",
    "    y_copy = y.copy()\n",
    "    y_copy[diff <= 0] = 0\n",
    "    gradient = W - C * np.dot(y_copy, X) / X.shape[0]\n",
    "    #print(\"Gradient\", gradient)\n",
    "    return gradient\n",
    "\n",
    "def output(X, W):\n",
    "    return np.dot(X, W)\n",
    "\n",
    "def BGD(iterate_number, W, X_train, y_train, x_test, y_test):\n",
    "    loss_train = []\n",
    "    loss_valid = []\n",
    "\n",
    "    for i in range(iterate_number):\n",
    "        gradient = get_gradient(X_train, W, y_train)\n",
    "        #print(gradient)\n",
    "        W = W - leraning_rate * gradient\n",
    "        loss_train.append(compute_loss(X_train, W, y_train))\n",
    "        loss_valid.append(compute_loss(x_test, W, y_test))\n",
    "    return loss_train, loss_valid\n",
    "\n",
    "def Adam(iterate_number, W, X_train, y_train, x_test, y_test, learning_rate):\n",
    "    beta_1 = 0.9\n",
    "    beta_2 = 0.99\n",
    "    ep = 0.00000001\n",
    "\n",
    "    m = 0  \n",
    "    v = 0  \n",
    "    t = 0  \n",
    "\n",
    "    loss_train = []\n",
    "    loss_valid = []\n",
    "\n",
    "    N = X_train.shape[0]\n",
    "\n",
    "    for i in range(iterate_number):\n",
    "        h = output(X_train, W)\n",
    "        error = h - y_train\n",
    "        gradient = (X_train.T * error) / N\n",
    "        t = t + 1\n",
    "        m = beta_1 * m + (1 - beta_1) * gradient\n",
    "        v = beta_2 * v + (1 - beta_2) * (np.power(gradient, 2))\n",
    "        mt = m / (1 - beta_1**t)\n",
    "        vt = v /(1 - (beta_2**t))\n",
    "\n",
    "        W = W - learning_rate * mt / (np.sqrt(vt) + ep)\n",
    "        loss_train.append(compute_loss(X_train, W, y_train))\n",
    "        loss_valid.append(compute_loss(x_test, W, y_test))\n",
    "    return loss_train, loss_valid\n",
    "\n",
    "def RMSProp(iterate_number, W, X_train, y_train, x_test, y_test, learning_rate):\n",
    "    N = X_train.shape[0]\n",
    "\n",
    "    d = 0.9\n",
    "\n",
    "    Egt=0  \n",
    "    Edt = 0\n",
    "    delta = 0\n",
    "    ep = 0.00000001 \n",
    "\n",
    "    loss_train = []\n",
    "    loss_valid = []\n",
    "\n",
    "    for i in range(iterate_number):\n",
    "        h = output(X_train, W)\n",
    "        error = h - y_train\n",
    "        gradient = (X_train.T * error) / N\n",
    "        Egt = d * Egt + (1 - d)*(np.power(gradient, 2))  \n",
    "    \n",
    "        W = W - learning_rate * gradient / (np.sqrt(Egt) + ep)\n",
    "        loss_train.append(compute_loss(X_train, W, y_train))\n",
    "        loss_valid.append(compute_loss(x_test, W, y_test))\n",
    "    return loss_train, loss_valid\n",
    "\n",
    "def ADADELTA(iterate_number, W, X_train, y_train, x_test, y_test):\n",
    "    N = X_train.shape[0]\n",
    "\n",
    "    ep = 0.00001\n",
    "\n",
    "    d = 0.9\n",
    "  \n",
    "    Egt = 0\n",
    "    Edt = 0\n",
    "    sumDelta = 0\n",
    "\n",
    "    loss_train = []\n",
    "    loss_valid = []\n",
    "\n",
    "    for i in range(iterate_number):\n",
    "        h = output(X_train, W)\n",
    "        error = h - y_train\n",
    "        gradient = (X_train.T * error) / N\n",
    "       \n",
    "        Egt = d * Egt + (1 - d) * (np.power(gradient, 2))\n",
    "        delta =  np.multiply(np.sqrt(Edt + ep), gradient) / np.sqrt(Egt + ep)\n",
    "        Edt = d * Edt + (1 - d) * (np.power(delta, 2))\n",
    "    \n",
    "        W = W - delta\n",
    "        loss_train.append(compute_loss(X_train, W, y_train))\n",
    "        loss_valid.append(compute_loss(x_test, W, y_test))\n",
    "    return loss_train, loss_valid\n",
    "\n",
    "def NAG(iterate_number, W, X_train, y_train, x_test, y_test, learning_rate):\n",
    "    N = X_train.shape[0]\n",
    "\n",
    "    u = 0.9\n",
    "  \n",
    "    vt = 0\n",
    "\n",
    "    loss_train = []\n",
    "    loss_valid = []\n",
    "\n",
    "    for i in range(iterate_number):\n",
    "        h = output(X_train, W + u * vt)\n",
    "        error = h - y_train\n",
    "        gradient = (X_train.T * error) / N\n",
    "       \n",
    "        vt = u * vt - learning_rate * gradient\n",
    "    \n",
    "        W = W + vt\n",
    "        loss_train.append(compute_loss(X_train, W, y_train))\n",
    "        loss_valid.append(compute_loss(x_test, W, y_test))\n",
    "    return loss_train, loss_valid\n",
    "    \n",
    "#ploting the loss value\n",
    "def plot(loss_train, loss_valid, title):\n",
    "    plt.plot(loss_train, color=\"r\", label=\"Loss_train\")\n",
    "    plt.plot(loss_valid, color=\"g\",label=\"Loss_valid\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "X_train, y_train = get_data(\"D:/Task/MachineLearning/Test/逻辑回归、线性分类和随机梯度下降/SVM_SGD/data/a9a\")\n",
    "x_test, y_test = get_data(\"D:/Task/MachineLearning/Test/逻辑回归、线性分类和随机梯度下降/SVM_SGD/data/a9a.t\")\n",
    "X_train = X_train.toarray()\n",
    "x_test = x_test.toarray()\n",
    "column_train = np.ones((X_train.shape[0]))\n",
    "X_train = np.column_stack((X_train, column_train))\n",
    "column_test = np.ones((x_test.shape[0]))\n",
    "x_test = np.column_stack((x_test, column_test))\n",
    "\n",
    "\n",
    "y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "y_test = y_test.reshape(y_test.shape[0], 1)\n",
    "y_train[y_train < 0] = 0\n",
    "y_test[y_test < 0] = 0\n",
    "\n",
    "N = X_train.shape[1]\n",
    "#initialize the parameters\n",
    "W = np.ones((N, 1))\n",
    "\n",
    "#set the learning rate and iterate number\n",
    "leraning_rate = 0.01\n",
    "iterate_number = 1000\n",
    "\n",
    "pic_title = \"Linear SVM\"\n",
    "loss_train, loss_valid = BGD(iterate_number, W, X_train, y_train, x_test, y_test)\n",
    "plot(loss_train, loss_valid, pic_title)\n",
    "\n",
    "#loss_train, loss_valid = SGD(iterate_number, W, X_train, y_train, x_test, y_test, learning_rate)\n",
    "pic_title = \"Adam\"\n",
    "loss_train, loss_valid = Adam(iterate_number, W, X_train, y_train, x_test, y_test, learning_rate)\n",
    "pic_title = \"RMSProp\"\n",
    "loss_train, loss_valid = RMSProp(iterate_number, W, X_train, y_train, x_test, y_test, learning_rate)\n",
    "pic_title = \"ADADELTA\"\n",
    "loss_train, loss_valid = ADADELTA(iterate_number, W, X_train, y_train, x_test, y_test)\n",
    "pic_title = \"NAG\"\n",
    "loss_train, loss_valid = NAG(iterate_number, W, X_train, y_train, x_test, y_test, learning_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
